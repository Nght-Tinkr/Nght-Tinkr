{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "macro-metropolitan",
   "metadata": {},
   "source": [
    "<h1>Advanced AI project: NLP</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-telescope",
   "metadata": {},
   "source": [
    "In this Jupyter notebook is my assignment which is to use data from a dataset which has song titles and genre linked and have an AI use it to sort a song into a specific genre based on the name of the song, it will do this by using a Neural Language Processor to classify the songs. Due to the limitations of not finding a perfect dataset for this it can only classify them into two genres, Rock and Hip-Hop, as the most appropriate dataset I found only contained songs from these two genres. Although this was not ideal it did make work slightly easier by being able to attribute numerals to the genres, 1 and 0, and allowing it to be treat as a binary/two-class classification.\n",
    "\n",
    "    \n",
    "The inputs of the system is the dataset with the name of 17735 songs with their corresponding genre, however as previously mentioned ideally there would be a dataset with more than just two genres allowing for a more accurate way of classification, sorting songs into more genres. The outputs will be the data after it has been classified into the genres. In the code you can see that there was created a validation set of data within the training set, this was done so that testing of accuracy of the model could be done as well as to tune it with an unseen set of data without using the test data which would be use afterwards for the evaluation part.\n",
    "\n",
    "    \n",
    "If worked on more there would’ve been a feature added to allow user input, this would be allowing the user to write in a song title in an input space and for the NLP to process it and classify that new title into the genre it thinks it should go in, giving a response to the user. There would’ve then been a way for the user to say if the NLP was correct or not, the model would then add that title to the pre-existing data if it was placed into the correct genre or allowing the user to write in the correct genre if wrong and it would then add it into the data, this would allow for continuous update without having to add the data manually into the dataset.\n",
    "\n",
    "    \n",
    "In the code there is a function that tests the previously separated test data and has it print out the result of the test giving a loss and accuracy score, the loss is a number that represents the error occurrence with lower numbers being better, the accuracy gives me a percentage of how accurate/ how correct the NLP was at classifying the songs into genres based on the title. The data would be plotted the into graphs, with one showing the training and validation loss over the Epochs and the other showing the training and validation accuracy over the Epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-richardson",
   "metadata": {},
   "source": [
    "<h2>Model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-neutral",
   "metadata": {},
   "source": [
    "In this next section the code will be broken up into smaller sections to give a brief description of the building process, the entire block of code will be displayed at the end together.\n",
    "\n",
    "Before starting on the implementation in Jupyter Notebook research into this area needed to be done, it was concluded that TensorFlow had great documentation and tutorials to help in creating this NLP, there was also help in trouble shooting problems found by looking at similar ones found on either stackoverflow.com or github.com. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-vacation",
   "metadata": {},
   "source": [
    "The first step taken to build this system was to import all of the initial dependencies and modules that were needed for the system to work, such as numpy, pandas, TensorFlow and matplotlib.pyplot, as more code was added other modules would be imported as and when they were needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-congress",
   "metadata": {},
   "source": [
    "The next step was to find and then implement the dataset, finding a dataset was quite difficult as there weren’t many that included song title as well as the genre, there were a few good ones but it wasn’t possible to get access to them or they were too convoluted to use such as Spotipy, the dataset was then implemented by using pd.read_csv. After this the data would need to split into train data and test data which was done by using the train_test_split function from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-tobago",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"SongGenreDataset.csv\")\n",
    "\n",
    "x = dataset_tf.values[:, 1]\n",
    "y = dataset_tf.values[:, 0]\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-williams",
   "metadata": {},
   "source": [
    "Once the dataset had been implemented and split the model had to be built, and through previously done research into TensorFlow the decision was made to use keras, keras is an API that is used to build and train models within TensorFlow. An optimiser was needed as well as a loss function and the ones used are the Adam optimiser and Binary Crossentropy to calculate the loss function, the use Binary Crossentropy over other methods is due to its ability at dealing with probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "hub_layer = hub.KerasLayer(model, input_shape=[], dtype=tf.string, trainable=True)\n",
    "hub_layer(xtrain[:3])\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-enterprise",
   "metadata": {},
   "source": [
    "The model has now been created so it now needs to be trained using the train data from the dataset, this train data was also split into some validation train data to allow testing and tuning of the model whilst it is training. After the model has been created and trained the testing needed to be done using the test data that had been split previously, doing so it will produce an output showing loss and accuracy. I am aiming for a loss of less than 0.2 and an accuracy of 0.9 or greater however predict that I should be getting a loss of below 0.5 and an accuracy greater than 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = xtrain[:1000]\n",
    "partial_x_train = xtrain[1000:]\n",
    "\n",
    "y_val = ytrain[:1000]\n",
    "partial_y_train = ytrain[1000:]\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=40,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-chart",
   "metadata": {},
   "source": [
    "There was an error in the final step of creating the model which halted progression entirely wouldn’t allow the model to be fully train which meant that it was unable to be tested and show results. In the main block of code you can see parts which are hashed out, those where different attempts to fix the error that was occurring, the problem was believed to be fairly simple however no solution was found at the time of submission.This is very frustrating as it meant that what was sought out to do with this project was not able to be achieved, however there will be data provided to show how the code would look like and how the results should look like including code for making and presentation of loss and accuracy graphs.\n",
    "\n",
    "The code that is immediately below shows you how the testing and results would’ve been done as well as an example of how that should’ve looked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-intersection",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(xtest, ytest)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "drawn-knife",
   "metadata": {},
   "source": [
    "<img src=\"results.PNG\" alt=\"Model testing Results\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-richards",
   "metadata": {},
   "source": [
    "A history object is returned by the model.fit function in the training stage, this shows everything that happened during training. That object is then used to create the two graphs shown below which where plotted using the matplotlib.pyplot function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-checkout",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-williams",
   "metadata": {},
   "source": [
    "The code and graph below show what the Training and validation accuracy would've looked like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-lambda",
   "metadata": {},
   "source": [
    "<img src=\"loss graph.PNG\" alt=\"Training and Validation Loss graph\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-identifier",
   "metadata": {},
   "source": [
    "The code and graph below show what the Training and validation loss would've looked like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-south",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-employment",
   "metadata": {},
   "source": [
    "<img src=\"Accuracy graph.PNG\" alt=\"Training and Validation Accuracy graph\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-duplicate",
   "metadata": {},
   "source": [
    "The block of code below is the entire code that was used not broken up so that an overview of the code can be seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "stable-external",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.4.1\n",
      "Eager mode:  True\n",
      "Hub version:  0.8.0\n",
      "GPU is NOT AVAILABLE\n",
      "({'title': <tf.Tensor: shape=(32, 32), dtype=string, numpy=\n",
      "array([[b'Universal Language GLAD2MECHA & FAYNT', b'Dark Ages',\n",
      "        b'Driving Down To Main Street (To Buy A New Hat)', ...,\n",
      "        b'(title unknown)', b'John Of The Sword',\n",
      "        b'Stronger Than Dirt/Liquor Break'],\n",
      "       [b'I Am I Said',\n",
      "        b'Bloc Party (Featuring CM aka Creative, The Honorable Sleaze, C-Doc & Mported Flows)',\n",
      "        b'No Change Our Hearts Shall Fear', ..., b'Nudity', b'Shithead',\n",
      "        b'Gestaagd'],\n",
      "       [b'Les dangers du stop', b'Ghost Heart', b'Corridors', ...,\n",
      "        b'You Know That I Will', b'Chrome Crucifix', b'Rage'],\n",
      "       ...,\n",
      "       [b'Antarctica', b'Current Seas', b'Kick Em Out', ...,\n",
      "        b'14 Souls of the Condemned ft. PRIVATE GOVERNMENT',\n",
      "        b'Skincrawler', b'Sometime'],\n",
      "       [b'Black Angel', b'post-rigodon', b'Up all night', ...,\n",
      "        b'SSG Live', b'Meltdown Momma', b'This Is Not A Road'],\n",
      "       [b'Mouse Trap', b'Sucking Leach', b'Picking Up a Bingo Chip', ...,\n",
      "        b'Mooks', b'ride out', b'Muscadine Wine']], dtype=object)>}, <tf.Tensor: shape=(32, 32), dtype=int64, numpy=\n",
      "array([[0, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 0, 1, ..., 1, 1, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 1, 1],\n",
      "       [1, 0, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 0, 1]], dtype=int64)>)\n",
      "12413\n",
      "12413\n",
      "5321\n",
      "5321\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x000001AC512B2E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x000001AC512B2E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x000001AC440B9EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x000001AC440B9EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Exception ignored in: <function CapturableResourceDeleter.__del__ at 0x000001ABEDE8C160>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py\", line 208, in __del__\n",
      "    self._destroy_resource()\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 871, in _call\n",
      "    self._initialize(args, kwds, add_initializers_to=initializers)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 725, in _initialize\n",
      "    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\n",
      "    graph_function, _ = self._maybe_define_function(args, kwargs)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3361, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3196, in _create_graph_function\n",
      "    func_graph_module.func_graph_from_py_func(\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 990, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 634, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\saved_model\\function_deserialization.py\", line 253, in restored_function_body\n",
      "    return _call_concrete_function(function, inputs)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\saved_model\\function_deserialization.py\", line 75, in _call_concrete_function\n",
      "    result = function._call_flat(tensor_inputs, function._captured_inputs)  # pylint: disable=protected-access\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 115, in _call_flat\n",
      "    return super(_WrapperFunction, self)._call_flat(args, captured_inputs,\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1932, in _call_flat\n",
      "    flat_outputs = forward_function.call(ctx, args_with_tangents)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 583, in call\n",
      "    outputs = functional_ops.partitioned_call(\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 1206, in partitioned_call\n",
      "    f.add_to_graph(graph)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 505, in add_to_graph\n",
      "    g._add_function(self)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3395, in _add_function\n",
      "    pywrap_tf_session.TF_GraphCopyFunction(self._c_graph, function._c_func.func,\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: 'func' argument to TF_GraphCopyFunction cannot be null\n",
      "Exception ignored in: <function CapturableResourceDeleter.__del__ at 0x000001ABEDE8C160>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py\", line 208, in __del__\n",
      "    self._destroy_resource()\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 871, in _call\n",
      "    self._initialize(args, kwds, add_initializers_to=initializers)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 725, in _initialize\n",
      "    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\n",
      "    graph_function, _ = self._maybe_define_function(args, kwargs)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3361, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3196, in _create_graph_function\n",
      "    func_graph_module.func_graph_from_py_func(\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 990, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 634, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\saved_model\\function_deserialization.py\", line 253, in restored_function_body\n",
      "    return _call_concrete_function(function, inputs)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\saved_model\\function_deserialization.py\", line 75, in _call_concrete_function\n",
      "    result = function._call_flat(tensor_inputs, function._captured_inputs)  # pylint: disable=protected-access\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 115, in _call_flat\n",
      "    return super(_WrapperFunction, self)._call_flat(args, captured_inputs,\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1932, in _call_flat\n",
      "    flat_outputs = forward_function.call(ctx, args_with_tangents)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 583, in call\n",
      "    outputs = functional_ops.partitioned_call(\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 1206, in partitioned_call\n",
      "    f.add_to_graph(graph)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 505, in add_to_graph\n",
      "    g._add_function(self)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3395, in _add_function\n",
      "    pywrap_tf_session.TF_GraphCopyFunction(self._c_graph, function._c_func.func,\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: 'func' argument to TF_GraphCopyFunction cannot be null\n",
      "Exception ignored in: <function CapturableResourceDeleter.__del__ at 0x000001ABEDE8C160>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py\", line 208, in __del__\n",
      "    self._destroy_resource()\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 871, in _call\n",
      "    self._initialize(args, kwds, add_initializers_to=initializers)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 725, in _initialize\n",
      "    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\n",
      "    graph_function, _ = self._maybe_define_function(args, kwargs)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3361, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3196, in _create_graph_function\n",
      "    func_graph_module.func_graph_from_py_func(\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 990, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 634, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\saved_model\\function_deserialization.py\", line 253, in restored_function_body\n",
      "    return _call_concrete_function(function, inputs)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\saved_model\\function_deserialization.py\", line 75, in _call_concrete_function\n",
      "    result = function._call_flat(tensor_inputs, function._captured_inputs)  # pylint: disable=protected-access\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 115, in _call_flat\n",
      "    return super(_WrapperFunction, self)._call_flat(args, captured_inputs,\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1932, in _call_flat\n",
      "    flat_outputs = forward_function.call(ctx, args_with_tangents)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 583, in call\n",
      "    outputs = functional_ops.partitioned_call(\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 1206, in partitioned_call\n",
      "    f.add_to_graph(graph)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 505, in add_to_graph\n",
      "    g._add_function(self)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3395, in _add_function\n",
      "    pywrap_tf_session.TF_GraphCopyFunction(self._c_graph, function._c_func.func,\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: 'func' argument to TF_GraphCopyFunction cannot be null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function CapturableResourceDeleter.__del__ at 0x000001ABEDE8C160>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py\", line 208, in __del__\n",
      "    self._destroy_resource()\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 871, in _call\n",
      "    self._initialize(args, kwds, add_initializers_to=initializers)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 725, in _initialize\n",
      "    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\n",
      "    graph_function, _ = self._maybe_define_function(args, kwargs)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3361, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3196, in _create_graph_function\n",
      "    func_graph_module.func_graph_from_py_func(\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 990, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 634, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\saved_model\\function_deserialization.py\", line 253, in restored_function_body\n",
      "    return _call_concrete_function(function, inputs)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\saved_model\\function_deserialization.py\", line 75, in _call_concrete_function\n",
      "    result = function._call_flat(tensor_inputs, function._captured_inputs)  # pylint: disable=protected-access\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 115, in _call_flat\n",
      "    return super(_WrapperFunction, self)._call_flat(args, captured_inputs,\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1932, in _call_flat\n",
      "    flat_outputs = forward_function.call(ctx, args_with_tangents)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 583, in call\n",
      "    outputs = functional_ops.partitioned_call(\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 1206, in partitioned_call\n",
      "    f.add_to_graph(graph)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 505, in add_to_graph\n",
      "    g._add_function(self)\n",
      "  File \"C:\\Users\\einar\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3395, in _add_function\n",
      "    pywrap_tf_session.TF_GraphCopyFunction(self._c_graph, function._c_func.func,\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: 'func' argument to TF_GraphCopyFunction cannot be null\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x000001AC512B28B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x000001AC512B28B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Python inputs incompatible with input_signature:\n  inputs: (\n    ({')\n  input_signature: (\n    TensorSpec(shape=(None,), dtype=tf.string, name=None))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-1955e47dda4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://tfhub.dev/google/nnlm-en-dim50/2\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[0mhub_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m \u001b[0mhub_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_xtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1012\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;31m# or else Keras' global `learning_phase`, which might actually be a tensor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36m_call_attribute\u001b[1;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_call_attribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    889\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m           self._stateful_fn._function_spec.canonicalize_function_inputs(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    892\u001b[0m               *args, **kwds)\n\u001b[0;32m    893\u001b[0m       \u001b[1;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcanonicalize_function_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2701\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2702\u001b[0m       \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2703\u001b[1;33m       inputs, flat_inputs, filtered_flat_inputs = _convert_inputs_to_signature(\n\u001b[0m\u001b[0;32m   2704\u001b[0m           inputs, self._input_signature, self._flat_input_signature)\n\u001b[0;32m   2705\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_convert_inputs_to_signature\u001b[1;34m(inputs, input_signature, flat_input_signature)\u001b[0m\n\u001b[0;32m   2803\u001b[0m       \u001b[0mflat_input_signature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2804\u001b[0m       flatten_inputs)):\n\u001b[1;32m-> 2805\u001b[1;33m     raise ValueError(\"Python inputs incompatible with input_signature:\\n%s\" %\n\u001b[0m\u001b[0;32m   2806\u001b[0m                      format_error_message(inputs, input_signature))\n\u001b[0;32m   2807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Python inputs incompatible with input_signature:\n  inputs: (\n    ({')\n  input_signature: (\n    TensorSpec(shape=(None,), dtype=tf.string, name=None))"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
    "\n",
    "\n",
    "dataset = pd.read_csv(\"SongGenreDataset.csv\")\n",
    "\n",
    "#dataset_tf = tf.convert_to_tensor(dataset, string)\n",
    "\n",
    "#df = pd.DataFrame(dataset, columns = ['genre','title'])\n",
    "\n",
    "x = dataset.values[:, 1]\n",
    "y = dataset.values[:, 0]\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#train, test = tfds.load(name=\"SongGenreDataset\", split=[\"title\", \"genre\"], batch_size=-1, as_supervised=True)\n",
    "#xtrain, ytrain = tfds.as_numpy(train)\n",
    "#xtest, ytest = tfds.as_numpy(test)\n",
    "\n",
    "df[\"genre\"] = df.genre.apply(str)\n",
    "\n",
    "def df_to_dataset(data, shuffle=True, batch_size=32):\n",
    "  data = dataset.copy()\n",
    "  labels = dataset.pop('genre')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataset), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataset))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds\n",
    "\n",
    "trainx = df_to_dataset(xtrain, batch_size=32)\n",
    "tf_xtrain = next(iter(trainx.batch(32)))  #iterating throught the batch one by one instead of finding an index\n",
    "tf_xtrain = str(tf_xtrain)\n",
    "\n",
    "#above code is above is turning the data into numpy_arrays so that tensorflow can read it properly\n",
    "\n",
    "print(tf_xtrain)\n",
    "# a problem was occuring so needed to see what was happening, it returns the dtype=int64 instead of string\n",
    "\n",
    "#tf_xtrain = tf.convert_to_tensor(xtrain, dtype=tf.string)\n",
    "#tf_ytrain = tf.convert_to_tensor(ytrain, dtype=tf.float32)\n",
    "\n",
    "\n",
    "print(len(xtrain))\n",
    "print(len(ytrain))\n",
    "print(len(xtest))\n",
    "print(len(ytest))\n",
    "#There was a problem early on in development and these values needed to be seen to understand what\n",
    "#and where things were going wrong\n",
    "\n",
    "model = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "hub_layer = hub.KerasLayer(model, input_shape=[], dtype=tf.string, trainable=True)\n",
    "hub_layer(tf_xtrain[:3])\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')])\n",
    "\n",
    "\n",
    "x_val = tf_xtrain[:1000]\n",
    "partial_x_train = xtrain[1000:]\n",
    "\n",
    "y_val = tf_ytrain[:1000]\n",
    "partial_y_train = ytrain[1000:]\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=40,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)\n",
    "\n",
    "#code below here would be showing the results had not there been an error\n",
    "\n",
    "results = model.evaluate(xtest, ytest)\n",
    "\n",
    "print(results)\n",
    "\n",
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-rating",
   "metadata": {},
   "source": [
    "<h2> Refrences </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-community",
   "metadata": {},
   "source": [
    "TensorFlow. 2021. Classify structured data with feature columns  |  TensorFlow Core. [online] Available at: <https://www.tensorflow.org/tutorials/structured_data/feature_columns> [Accessed 1 February 2021].\n",
    "\n",
    "Medium. 2021. Music Genre Classification with Python. [online] Available at: <https://towardsdatascience.com/music-genre-classification-with-python-c714d032f0d8> [Accessed 1 February 2021].\n",
    "\n",
    "Medium. 2021. Musical Genre Classification with Convolutional Neural Networks. [online] Available at: <https://towardsdatascience.com/musical-genre-classification-with-convolutional-neural-networks-ff04f9601a74> [Accessed 1 February 2021].\n",
    "\n",
    "Medium. 2021. Natural Language Processing with Tensorflow. [online] Available at: <https://towardsdatascience.com/natural-language-processing-with-tensorflow-e0a701ef5cef> [Accessed 1 February 2021].\n",
    "\n",
    "GitHub. 2021. reachanihere/Song-Genre-Classification. [online] Available at: <https://github.com/reachanihere/Song-Genre-Classification/blob/master/datasets/fma-rock-vs-hiphop.csv> [Accessed 1 February 2021].\n",
    "\n",
    "Scikit-learn.org. 2021. sklearn.model_selection.train_test_split — scikit-learn 0.24.1 documentation. [online] Available at: <https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html> [Accessed 1 February 2021].\n",
    "\n",
    "TensorFlow. 2021. Text Classification with Movie Reviews  |  TensorFlow Hub. [online] Available at: <https://www.tensorflow.org/hub/tutorials/tf2_text_classification#more_models> [Accessed 1 February 2021].\n",
    "\n",
    "Medium. 2021. Gentle Start to Natural Language Processing using Python. [online] Available at: <https://towardsdatascience.com/gentle-start-to-natural-language-processing-using-python-6e46c07addf3#:~:text=Natural%20language%20toolkit%20(NLTK)%20is,library%20that%20you'll%20use.> [Accessed 1 February 2021]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-egypt",
   "metadata": {},
   "source": [
    "<h2>Appendix</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-agency",
   "metadata": {},
   "source": [
    "TensorFlow. 2021. Load a pandas.DataFrame  |  TensorFlow Core. [online] Available at: <https://www.tensorflow.org/tutorials/load_data/pandas_dataframe> [Accessed 1 February 2021].\n",
    "\n",
    "TensorFlow. 2021. NumPy API on TensorFlow  |  TensorFlow Core. [online] Available at: <https://www.tensorflow.org/guide/tf_numpy> [Accessed 1 February 2021].\n",
    "\n",
    "TensorFlow. 2021. NumPy API on TensorFlow  |  TensorFlow Core. [online] Available at: <https://www.tensorflow.org/guide/tf_numpy> [Accessed 1 February 2021].\n",
    "\n",
    "Pandas.pydata.org. 2021. pandas.DataFrame — pandas 1.2.1 documentation. [online] Available at: <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html> [Accessed 1 February 2021].\n",
    "\n",
    "TensorFlow. 2021. tf.convert_to_tensor  |  TensorFlow Core v2.4.1. [online] Available at: <https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor> [Accessed 1 February 2021].\n",
    "\n",
    "TensorFlow. 2021. tf.data: Build TensorFlow input pipelines  |  TensorFlow Core. [online] Available at: <https://www.tensorflow.org/guide/data> [Accessed 1 February 2021].\n",
    "\n",
    "TensorFlow. 2021. tf.keras.utils.get_file  |  TensorFlow Core v2.4.1. [online] Available at: <https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file> [Accessed 1 February 2021].\n",
    "\n",
    "TensorFlow. 2021. tf.numpy_function  |  TensorFlow Core v2.4.1. [online] Available at: <https://www.tensorflow.org/api_docs/python/tf/numpy_function> [Accessed 1 February 2021]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-nowhere",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
